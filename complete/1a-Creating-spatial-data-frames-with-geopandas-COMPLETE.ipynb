{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44501a00",
   "metadata": {},
   "source": [
    "# Spatial Dataframes 1: Creating them using GeoPandas\n",
    "ENV 859 - Fall 2022  \n",
    "© John Fay, Duke University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e78d0",
   "metadata": {},
   "source": [
    "### What is a spatial dataframe\n",
    "A **spatial dataframe** (aka a **geodataframe** or **spatially enabled dataframe**) is much like a typical Pandas dataframe except that it accomodates a new datatype:  ***geometries*** . Geometries, as you might guess represent geometric features: points, lines, and polygons -- each of which is defined by a one or series of coordinate pairs. These spatial dataframes are also assigned a **coordinate reference system (crs)** which links these coordinates to specific places on the Earth and allows us to do geospatial analysis. In other words, these spatial dataframes are quite the same as our familiar GIS feature classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b314b83",
   "metadata": {},
   "source": [
    "### Libraries for working with spatial dataframes\n",
    "To work with spatial dataframes, we need one of two Python libraries, each of which has its own version of the spatial dataframe and its own set of functions and classes. First is **geopandas**, which has been around for a while, and then there's the newcomer, the **ArcGIS API for Python** (which really needs a better name). In this notebook we explore the former, **geopandas**, focusing in on how we create the GeoDataframes from existing data in various formats. The ArcGIS API for Python will be examined in a different notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897ea58",
   "metadata": {},
   "source": [
    "## Lesson 1 - Constructing Spatial Dataframes\n",
    "Before seeing what we can do with spatial dataframes, we need to learn how to construct them in our coding environment. So, our first lesson examines just that. Spatial data can be stored in various formats, and here we look at the methods for importing data stored in the more common formats into a GeoPandas spatial dataframe -- often called a ***geodataframe***. \n",
    "\n",
    "The source formats we examine include:\n",
    "1. [A delimited text file (e.g. CSV) containing coordinate columns and a know coordinate reference system](#1.1---Creating-spatial-dataframes-from-CSV-files-using-GeoPandas)\n",
    "2. [An existing feature class in the form of a shapefile or within a geodatabase](#1.2:-Creating-spatial-dataframes-from-existing-feature-classes)\n",
    "3. [Other formats: GeoJSON files, KML, and [kind of] GeoDatabases](#1.3---Creating-spatial-dataframes-from-other-file-formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3b826",
   "metadata": {},
   "source": [
    "### 1.1 - Creating spatial dataframes from CSV files using GeoPandas\n",
    "In this example, we examine how to create a point spatial dataframe from a CSV file containing latitude and longitude coordinates. The data we'll use in this exercise is electric vehicle charging locations in North Carolina obtained from the Alternative Fuels Data Center ([link](https://afdc.energy.gov/data_download)). These data, stored in the `NC_Charging_Stations.csv` file in the `data` folder, have been downloaded and preprocesed to subset records falling within North Carolina, and to remove extraneous columns.\n",
    "\n",
    "The process of importing a CSV file into a GeoPandas geodataframe consists of first importing the data into a Pandas dataframe and then creating a **GeoSeries** - or column of geometry objects - from the coordinate columns. Then we construct the geodataframe using the GeoPandas `GeoDataFrame()` function supplying the original dataframe, the geoseries object, and the coordinate reference system or `crs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858e8f0",
   "metadata": {},
   "source": [
    "#### Step 1. Importing the data into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries: Pandas (as \"pd\") and geopandas (as \"gpd\")\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae16e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the EV Charging station data into a Pandas dataframe\n",
    "df = pd.read_csv('../data/NC_Charging_Stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the first few rows, noting the data include \"latitude\"  \"longitude\" columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5616d",
   "metadata": {},
   "source": [
    "#### Step 2. Creating a column of geometric objects (i.e., a GeoSeries) \n",
    "To create a geoseries, we use the geopandas `points_from_xy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show info on the command\n",
    "gpd.points_from_xy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ea417",
   "metadata": {},
   "source": [
    "The essential inputs are a series of x coordinates (our `Longitude` column), a series of y coordinates (our `Latitude` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1525c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a geoseries object from the coordinate column\n",
    "geometries = gpd.points_from_xy(\n",
    "    x=df['Longitude'],\n",
    "    y=df['Latitude']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a359b9",
   "metadata": {},
   "source": [
    "#### Step 3. Constructing the Geodataframe from the data, the geoseries, and the crs\n",
    "Next, we use the `GeoDataFrame()` function to construct our geodataframe, attaching our geoseries as its \"shape\" field. We also, however, need to define the geodataframes's coordinate reference system, which is done by specifing the *well known ID* or **WKID** (really?) of the coordinate system to which our data is referenced.  \n",
    "\n",
    ">#### ► What is an WKID code?\n",
    ">All \"official\" coordinate systems have a unique ID, often defined by the \"European Petroleum Survey Group\". These ids, often refered to as  \"***WKIDs***\" (short for \"Well Known IDs\"), or sometimes as \"***EPSG codes***\", can be found by looking up the name of the coordinate system on either https://spatialreference.org or https://epsg.io/. For example, the WKID for WGS 84 (to which is what our data is referenced) is [4326](https://spatialreference.org/ref/epsg/wgs-84/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719de7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the GeoDataframe() command syntax\n",
    "gpd.GeoDataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a geodataframe from our data\n",
    "gdf_csv = gpd.GeoDataFrame(\n",
    "    data=df,\n",
    "    geometry=geometries,\n",
    "    crs = 4326\n",
    ")\n",
    "#Confirm the type of the object we just created\n",
    "type(gdf_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46037d93",
   "metadata": {},
   "source": [
    "#### Step 4. Exploring our geodataframe\n",
    "Now, let's explore our geodataframe using many commands familiar with our exploration of Pandas dataframes. These include:\n",
    "* `head()` to show the first few records of the dataframe (note the last column)\n",
    "* `info()` to reveal the structure of the dataframe (note the data type of the last column)\n",
    "* `crs` to reveal the coordinate reference system the dataset uses\n",
    "* `plot()` to plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb03212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the first few records of the geodataframe\n",
    "gdf_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the structure of the dataframe\n",
    "gdf_csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826722d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the geodataframe's coordinate reference system\n",
    "gdf_csv.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show just the EPSG code of the crs\n",
    "gdf_csv.crs.to_epsg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53939654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project to UTM Zone 17N \n",
    "gdf_utm = gdf_csv.to_csv(32617)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66376b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data\n",
    "gdf_csv.plot(color='Green', marker='x', alpha=0.3, figsize=(12,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0363ff",
   "metadata": {},
   "source": [
    "And that's it! Pretty straight forward. Soon we will explore the various analyses and visualizations we can do with these spatial dataframes, but first, we'll examine a few other types of data we can import into our coding environment as geodataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc98ca",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1.2: Creating spatial dataframes from existing feature classes\n",
    "Here we look at the process of getting existing feature classes, e.g. Shapefiles, into spatial dataframes. We'll again look at methods using GeoPandas and then compare that with similar methods using the ArcGIS API for Python. \n",
    "\n",
    "The dataset we'll use represents major river basins of North Carolina (source: https://data-ncdenr.opendata.arcgis.com/datasets/ncdenr::major-river-basins), a copy of which has been downloaded into the data folder as `Major_Basins.shp`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58819a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Step 1. Importing shapefiles using `read_file()`\n",
    "Importing feature classes using GeoPandas is easy with the `read_file()` command. \n",
    "\n",
    ">What's worth noting is that GeoPandas actually uses the Python **Fiona** package to read the shapefiles. Fiona leverages a collection of drivers that provide access to a number of geospatial data formats. Geopandas simplifies the usage of Fiona commands, making import and export of geodataframes easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a12284",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Explore the read_file() command\n",
    "gpd.read_file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c994ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Read the shapefile into a GeoPandas geodataframe\n",
    "gdf_shp = gpd.read_file('../data/Major_Basins.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7672a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Pro tip** -- a shapefile zipped into a single file can also be read in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daff34d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Read a *zipped* shapefile into a GeoPandas geodataframe\n",
    "gdf_shp = gpd.read_file('../data/Major_River_Basins.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9afb41",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Examine the data\n",
    "gdf_shp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf60e6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#What is the crs of the data\n",
    "gdf_shp.crs.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a00fbd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Plot the data...\n",
    "gdf_shp.plot(column='Basin', categorical=True, figsize=(12,7), cmap='Pastel2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa4ca4",
   "metadata": {},
   "source": [
    "### 1.3 - Creating spatial dataframes from other file formats\n",
    "\n",
    "Now we look at some formats that may be less familiar to you but are becoming more and more common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84676a",
   "metadata": {},
   "source": [
    "#### 1.3.1 - Reading GeoJSON files\n",
    "We have a GeoJSON format of the major river basins in NC saved in our data folder: `../data/12-Major_River_Basins.geojson` ([source](https://data-ncdenr.opendata.arcgis.com/datasets/ncdenr::major-river-basins/)). Let's see how we go about importing that file. \n",
    "\n",
    ">##### What is GeoJSON?\n",
    ">GeoJSON is a text based format that stores spatial features in a long, but universally readable format (i.e. text!). \"JSON\" stands for JavaScript Object Notation, and if you look at raw JSON files from a Python perspective, it looks like a set of nested dictionary and list objects. We need not get too deep into that, but understand that being text based, JSON and its spatial counterpart GeoJSON, are used widely in web-based services and can be quite useful in certain circumstances.\n",
    "\n",
    "As it happens, that Fiona package we read about, the one GeoPandas uses, can read this format as well. We simply have to indicate what **driver** the `read_file()` function should use to conver the file into a geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b074272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the file \n",
    "gdf_geojson = gpd.read_file(\n",
    "    filename='../data/Major_River_Basins.geojson',\n",
    "    driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_geojson.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2c364",
   "metadata": {},
   "source": [
    "#### 1.3.2 - Reading KML files\n",
    "We have a GeoJSON format of the major river basins in NC saved in our data folder: `../data/12-Major_River_Basins.geojson` ([source](https://data-ncdenr.opendata.arcgis.com/datasets/ncdenr::major-river-basins/)). Let's see how we go about importing that file. \n",
    "\n",
    ">##### ► What is KML?\n",
    ">KML, short for \"Keyhole Markup Language\", is yet another text based format developed to store geospatial features. This format was originally desgined to work with the Google Earth application (which was originally developed by a company called Keyhole), but others have adopted this format as well because of its simplicity. \n",
    "\n",
    "And yes, Fiona has a driver to work with KML files, but for some reason this driver is not enabled by default. Let's look at all the drivers Fiona can work with by default and how to enable this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Fiona\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e07c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display fiona's active drivers\n",
    "fiona.supported_drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebaa324",
   "metadata": {},
   "source": [
    "The result is a dictionary where the keys are the driver and the values are what we can do with them: \n",
    "* `r` indicates we can read those formats but not write to them\n",
    "* `rw` indicates we can both read from and write to those formats\n",
    "* `raw` indicates we can read, write, and append data to existing files in that format\n",
    "\n",
    "You'll also notice KML does not appear on that list, but we can add it with the code below (where we add it directly to the list the geopandas can see...) What is the complete list of drivers? For some odd reason, that's not easily found, but you can decifer a bit from this page: https://github.com/Toblerity/Fiona/blob/master/fiona/drvsupport.py. (Thanks to this [StackExchange page](https://gis.stackexchange.com/questions/191365/drivers-of-fiona) for revealing this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58bade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable the KML driver in geopandas as a read-write format\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ea255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the IML file\n",
    "gdf_kml = gpd.read_file('../data/Major_River_Basins.kml',driver='KML')\n",
    "gdf_kml.plot(column='Name');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33463c3c",
   "metadata": {},
   "source": [
    "#### 1.3.3 Reading ESRI Geodatabase files\n",
    "The ESRI Geodatabase is a tricky format that sits somewhere in the gray area between proprietary and opensource. ESRI does publish enough of how these Geodatabases are structured, programmatically, but that structure evolves quickly -- sometimes faster than coders can update Fiona drivers. \n",
    "\n",
    "For example: <https://github.com/Toblerity/Fiona/issues/428>\n",
    "\n",
    "In any event, those drivers are usually labeled as `OpenFileGDB` and you'd be best off doing a websearch for the latest sequence of commands required to read geodatabase feature classes into a spatial dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
